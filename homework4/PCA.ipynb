{"cells":[{"cell_type":"code","execution_count":1,"id":"4c97bfa5","metadata":{"id":"4c97bfa5","outputId":"6f356536-0eee-494e-fb73-88bbbe94b45f"},"outputs":[{"name":"stdout","output_type":"stream","text":["importing Jupyter notebook from Data_preprocess.ipynb\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import import_ipynb\n","from Data_preprocess import *\n"]},{"cell_type":"markdown","id":"0bb17ac2","metadata":{"id":"0bb17ac2"},"source":["### Steps of PCA:\n","\n","1. **Centeralize the Data:**\n","   - Centeralize the dataset by subtracting the mean\n","   $$X_{\\text{cent}} = X - \\bar{X}$$\n","\n","2. **Calculate the Covariance Matrix:**\n","   - Calculate the covariance matrix, which represents the relationships between different features.\n","   $$ \\text{Cov}(X, Y) = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\bar{X})(y_i - \\bar{Y})\\ $$\n","\n","3. **Compute Eigenvectors and Eigenvalues:**\n","   - Calculate the eigenvectors and eigenvalues of the covariance matrix.\n","   $$ \\text{Covariance Matrix} \\times \\text{Eigenvector} = \\text{Eigenvalue} \\times \\text{Eigenvector} $$\n","\n","4. **Sort Eigenvectors by Eigenvalues:**\n","   - Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n","\n","5. **Select Principal Components:**\n","   - Choose the top $k$ eigenvectors (principal components) corresponding to the $k$ largest eigenvalues to form the projection matrix $W$.\n","   $$ W = \\begin{bmatrix} \\text{eigenvector}_1 & \\text{eigenvector}_2 & \\ldots & \\text{eigenvector}_k \\end{bmatrix} $$\n","\n","6. **Transform the Data:**\n","   - Project the original data onto the new feature subspace using the projection matrix $W$.\n","   $$ \\text{Transformed Data} = \\text{Original Data} \\times W $$\n","\n","### Formula Notes:\n","- $n$ is the number of data points.\n","- $X$ and $Y$ are two variables.\n","- $\\bar{X}$ and $\\bar{Y}$ are the means of $X$ and $Y$, respectively.\n","- $\\text{Eigenvector}$ and $\\text{Eigenvalue}$ are obtained from the covariance matrix."]},{"cell_type":"code","execution_count":2,"id":"6e42be8b","metadata":{"id":"6e42be8b"},"outputs":[],"source":["class MY_PCA():\n","    '''\n","    Please implement the PCA function here\n","\n","    fit_transform -- Fit the model with input data and apply the dimensionality reduction on it.\n","    transform -- Apply dimensionality reduction to input data\n","    components_remain_ratio -- Calculate the minimum number of components needed to retain a specific proportion of the original data's information.\n","    reconstructData -- Reconstruct the data from top k eigenvectors\n","\n","    '''\n","    def __init__(self, n_components=2):\n","        self.n_components = n_components\n","        self.data = None\n","        self.eigen_vec = None\n","        self.eigen_val = None\n","        self.principal_components = None\n","        self.covariance_matrix = None\n","\n","\n","    @staticmethod\n","    def PCA_visualization(data_pca, label, text=False, n_components=2, tag=None):\n","\n","        plt.figure(figsize=(12, 4))\n","\n","        if len(label.shape) > 1:\n","            label = np.argmax(label, axis=1)\n","#         Plot the data in the reduced dimensional space\n","        if text:\n","            x_min, x_max = np.min(data_pca, 0), np.max(data_pca, 0)\n","            data_pca = (data_pca - x_min) / (x_max - x_min)\n","            plt.figure(figsize=(10, 6))\n","            for i in range(data_pca.shape[0]):\n","                if n_components == 2:\n","                    plt.text(data_pca[i, 0], data_pca[i, 1], str(label[i]),\n","                             color=plt.cm.Set1(label[i]),\n","                             fontdict={'size': 15})\n","                elif n_components == 1:\n","                    plt.text(data_pca[i], np.zeros(data_pca.shape[0]), str(label[i]),\n","                             color=plt.cm.Set1(label[i]),\n","                             fontdict={'size': 15})\n","\n","            plt.xticks([]), plt.yticks([]), plt.ylim([-0.1,1.1]), plt.xlim([-0.1,1.1])\n","\n","        else:\n","            if n_components == 2:\n","                plt.scatter(data_pca[:, 0], data_pca[:, 1], c=label, cmap='viridis', alpha=0.7)\n","            elif n_components == 1:\n","                plt.scatter(data_pca, np.zeros(data_pca.shape[0]), c=label, cmap='viridis', alpha=0.7)\n","\n","            plt.colorbar(label='Label')\n","            plt.tight_layout()\n","\n","        plt.title(f'{tag} Data in Reduced Dimensional Space (PCA) with Colored Labels')\n","        plt.xlabel('Principal Component 1')\n","        plt.ylabel('Principal Component 2')\n","        plt.savefig(f'PCA_{tag}.png')\n","        plt.show()\n","        plt.close()\n","\n","        return\n","\n","    def _COVARIANCE_MATRIX_COMPUTATION(self, centerized_data):\n","\n","        # GRADED CODE: Implement covariance matrix computation. (10%)\n","        ### !!! In this part, you can only use the common matrix operation in numpy. !!! ###\n","        ### !!! Please Do Not use another functions or libraries in this part. !!! ###\n","\n","        ### START CODE HERE ###\n","        num_samples = centerized_data.shape[0]\n","        self.covariance_matrix = np.dot(centerized_data.T, centerized_data) / (num_samples - 1)\n","        #self.covariance_matrix = np.dot(centerized_data.T, centerized_data)\n","        ### END CODE HERE ###\n","        return self.covariance_matrix\n","\n","\n","    def _COMPUTE_THE_EIGENVECTORS_AND_EIGENVALUES(self, cov_mat):\n","        '''\n","        cov_mat -> covariance matrix of input data\n","        eigen_val, eigen_vec -> the eigen value and coresponding eigen vector of covariance matrix.\n","        sorted_indices -> the sorted indices of eigen value in descending order\n","        '''\n","        # GRADED CODE: Calculate eigenvectors and eigenvalues.  (10%)\n","        ### !!! In this part, you can use any numpy functions. !!! ###\n","        ### !!! If you are using the numpy library, it is recommended to use np.linalg.eigh(). !!!###\n","\n","        ### START CODE HERE ###\n","        eigen_val, eigen_vec = np.linalg.eigh(cov_mat)\n","        sorted_indices = np.argsort(eigen_val)[::-1]\n","        self.eigen_val = eigen_val[sorted_indices]\n","        self.eigen_vec = eigen_vec[:, sorted_indices]\n","        ### END CODE HERE ###\n","\n","        return\n","\n","    def components_remain_ratio(self, ratio):\n","        '''\n","        Purpose: Calculate the minimum number of components needed to retain a specific proportion of the original data's information.\n","\n","        total_var -> sum of the variance of each component\n","        cumulative_var -> the cumulative sum of the var_ratio\n","        var_ratio -> percentage of variance contained of each principal component\n","        num_components -> the minimum number of principal components to cover 80% variance\n","        '''\n","        # GRADED CODE: RECONSTRUCT DATA\n","        ### START CODE HERE ###\n","        total_var = np.sum(self.eigen_val)\n","        var_ratio = self.eigen_val / total_var\n","        cumulative_var = np.cumsum(var_ratio)\n","        num_components = np.argmax(cumulative_var >= ratio) + 1\n","        return num_components, var_ratio\n","        ### END CODE HERE ###\n","\n","\n","    def reconstructData(self, data_x, mean_data, k):\n","        '''\n","        Purpose:  Reconstruct the data from top k eigenvectors\n","\n","        reconstruct_data -- the data reconstructed by top k eigenvector\n","        z -- the list of coefficients for top k eigenvector\n","        '''\n","        # GRADED CODE: RECONSTRUCT DATA\n","        ### START CODE HERE ###\n","        top_k_eigenvectors = self.eigen_vec[:, :k]\n","        z = np.dot(data_x - mean_data, top_k_eigenvectors)\n","        reconstruct_data = np.dot(z, top_k_eigenvectors.T) + mean_data\n","        ### END CODE HERE ###\n","\n","        return reconstruct_data, z\n","\n","    def transform(self, data_X):\n","        '''\n","        Purpose: Apply dimensionality reduction to input data\n","\n","        data_X -> The input data to apply the dimensionality reduction on it.\n","        data_pca -> the data after appling the dimensionality reduction.\n","        '''\n","        # GRADED CODE: PCA TRANSFORM FUNCTION\n","        ### START CODE HERE ###\n","        #mean_data = np.sum(self.data, axis=0) / self.data.shape[0]\n","        mean_data = np.mean(self.data, axis=0)\n","        centerized_data = data_X - mean_data\n","        data_pca = np.dot(centerized_data, self.principal_components)\n","        ### END CODE HERE ###\n","\n","        return data_pca\n","\n","    def fit_transform(self, data_X):\n","        '''\n","        Purpose: Fit the model with input data and apply the dimensionality reduction on it.\n","\n","        data_X -> The input data to fit model and apply the dimensionality reduction on it.\n","        self.data -> set the input data\n","        covariance_matrix -> the covarianve matrix of the data_X\n","        self.principal_components -> the principal components of data_X\n","        data_pca -> the data after appling the dimensionality reduction.\n","        '''\n","\n","        # GRADED CODE: PCA FITTING FUNCTION\n","        ### START CODE HERE ###\n","        self.data = data_X\n","        mean_data = np.mean(data_X, axis=0)\n","        #mean_data = np.sum(data_X, axis=0) / data_X.shape[0]\n","        centerized_data = data_X - mean_data\n","        self.covariance_matrix = self._COVARIANCE_MATRIX_COMPUTATION(centerized_data)\n","        self._COMPUTE_THE_EIGENVECTORS_AND_EIGENVALUES(self.covariance_matrix)\n","        self.principal_components = self.eigen_vec[:, :self.n_components]\n","        data_pca = np.dot(centerized_data, self.principal_components)\n","        ### END CODE HERE ###\n","\n","        return data_pca\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"3d92ce68","metadata":{"id":"3d92ce68"},"source":["### Steps of Sparse PCA:\n","\n","1. **Centeralize the Data:**\n","   - Centeralize the dataset by subtracting the mean\n","   \n","   $$X_{\\text{cent}} = X - \\bar{X}$$\n","\n","2. **Initialize Components:**\n","   - - Initialize the loading matrix $V$ using the transposed loading matrix from SVD.\n","   $$ V \\text{ (Initialize)} $$\n","\n","3. **Iterative Thresholding:**\n","   - Perform iterative thresholding to enforce sparsity on the loading matrix $V$.\n","\n","      $$ V_{\\text{new}} = \\text{soft\\_threshold}(V_{\\text{old}}, \\alpha) $$\n","      where $$ \\text{soft\\_threshold}(x, \\alpha) = \\text{sign}(x) \\cdot \\max(|x| - \\alpha, 0) $$\n","\n","4. **Normalize Components:**\n","   - Normalize the loading matrix $V$ to maintain unit length.\n","\n","     $$ V = \\frac{V}{\\|V\\|_2} $$\n","\n","5. **Repeat Iterations:**\n","   - Repeat steps 3-4 for a specified number of iterations or until convergence based on the change in sum of squared differences (SSD).\n","\n","6. **Transform the Data:**\n","   - Project the original data onto the sparse principal components $V$.\n","\n","      $$ \\text{Transformed Data} = \\text{Original Data} \\cdot V $$\n","\n","### Formula Notes:\n","- $X$ is the original data matrix.\n","- $X_{\\text{cent}}$ is the centerized data matrix.\n","- $V$ is the matrix of sparse principal components.\n","- $\\alpha$ is the sparsity-inducing parameter.\n","- $\\text{soft\\_threshold}(x, \\alpha)$ is the soft thresholding function.\n","- $\\text{sign}(x)$ returns the sign of $x$.\n","- $\\|\\cdot\\|_2$ denotes the L2 norm.\n"]},{"cell_type":"code","execution_count":3,"id":"ac49e182","metadata":{"id":"ac49e182"},"outputs":[],"source":["class MY_SparsePCA():\n","    def __init__(self, n_components, alpha, max_iter=1000, tol=1e-6):\n","        self.alpha = alpha\n","        self.n_components = n_components\n","        self.max_iter = max_iter\n","        self.tol = tol\n","        self.components_ = None\n","        self.error_ = None\n","        self.S = None\n","        self.Vt = None\n","\n","    @staticmethod\n","    def soft_threshold(x, alpha):\n","        # GRADED CODE: Iterative Thresholding\n","        ### START CODE HERE ###\n","        # Soft threshold function used for SPCA.\n","        rt = np.sign(x) * np.maximum(np.abs(x) - alpha, 0)\n","        ### END CODE HERE ###\n","\n","        return rt\n","\n","    def fit_transform(self, x):\n","\n","        # GRADED CODE: SPARSE PCA FITTINT FUNCTION\n","        ### START CODE HERE ###\n","        # Initialize components (5%)\n","        _, S, Vt = np.linalg.svd(x, full_matrices=False)\n","        self.components_ = Vt[:self.n_components, :].T\n","        self.S = S\n","        self.Vt = Vt\n","\n","        for iteration in tqdm(range(self.max_iter)):\n","\n","            # Compute the transform in this iteration\n","            x_projected = x @ self.components_\n","\n","            # Iterative Thresholding (update the components) (5%):\n","            self.components_ = self.soft_threshold(self.components_, self.alpha)\n","\n","            # Normalize components (5%)\n","            norm = np.linalg.norm(self.components_, axis=0)\n","            norm[norm < 1e-10] = 1.0\n","            self.components_ /= norm\n","\n","            # SSD Check\n","            ssd = np.sum((x - x_projected @ self.components_.T)**2)\n","            if ssd < self.tol:\n","                break\n","        # Transform the data (5%)\n","        x_transformed = x @ self.components_\n","\n","        ### END CODE HERE###\n","\n","        return x_transformed\n","\n","    def fit(self, x):\n","        self.fit_transform(x)\n","        return self\n","\n","    def transform(self, x):\n","        return x@self.components_\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.11.3 ('biopsy')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"bc07a8c6aa785ccbb5cb0815abffaffb139b111aca417c4ffe9bf221bae76ba2"}}},"nbformat":4,"nbformat_minor":5}
