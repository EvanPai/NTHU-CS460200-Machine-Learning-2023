{"cells":[{"cell_type":"markdown","id":"26b64234","metadata":{"id":"26b64234"},"source":["# Model\n",">* You can follow your work in HW3 to complete this file\n",">* You can not modify the initialize_parameters in Model Class"]},{"cell_type":"code","execution_count":1,"id":"69f78d91","metadata":{"id":"69f78d91"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","import math"]},{"cell_type":"code","execution_count":2,"id":"e61eeacf","metadata":{"id":"e61eeacf"},"outputs":[],"source":["class Dense():\n","    def __init__(self, n_x, n_y, seed=1):\n","        self.n_x = n_x\n","        self.n_y = n_y\n","        self.seed = seed\n","        self.initialize_parameters()\n","\n","    def initialize_parameters(self):\n","        \"\"\"\n","        Argument:\n","        self.n_x -- size of the input layer\n","        self.n_y -- size of the output layer\n","        self.parameters -- python dictionary containing your parameters:\n","                           W -- weight matrix of shape (n_x, n_y)\n","                           b -- bias vector of shape (1, n_y)\n","        \"\"\"\n","        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n","        np.random.seed(self.seed)\n","        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n","        b = np.zeros((1, self.n_y))\n","\n","        assert(W.shape == (self.n_x, self.n_y))\n","        assert(b.shape == (1, self.n_y))\n","\n","        self.parameters = {\"W\": W, \"b\": b}\n","\n","    def forward(self, A):\n","        \"\"\"\n","        Implement the linear part of a layer's forward propagation.\n","\n","        Arguments:\n","        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n","        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","\n","        Returns:\n","        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n","        \"\"\"\n","\n","        ### START CODE HERE ### (≈ 2 line of code)\n","        Z = np.dot(A, self.parameters[\"W\"]) + self.parameters[\"b\"]\n","        self.cache = (A, self.parameters[\"W\"], self.parameters[\"b\"])\n","        ### END CODE HERE ###\n","\n","        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n","\n","        return Z\n","\n","    def backward(self, dZ):\n","        \"\"\"\n","        Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","        Arguments:\n","        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n","        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n","        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n","\n","        Returns:\n","        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n","\n","        \"\"\"\n","        A_prev, W, b = self.cache\n","        m = A_prev.shape[0]\n","\n","        ### START CODE HERE ### (≈ 3 lines of code)\n","        self.dW = (1/m) * np.dot(A_prev.T, dZ)\n","        self.db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n","        dA_prev = np.dot(dZ, W.T)\n","        ### END CODE HERE ###\n","\n","        assert (dA_prev.shape == A_prev.shape)\n","        assert (self.dW.shape == self.parameters[\"W\"].shape)\n","        assert (self.db.shape == self.parameters[\"b\"].shape)\n","\n","        return dA_prev\n","\n","    def update(self, learning_rate):\n","        \"\"\"\n","        Update parameters using gradient descent\n","\n","        Arguments:\n","        learning rate -- step size\n","        \"\"\"\n","\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        self.parameters[\"W\"] -= learning_rate * self.dW\n","        self.parameters[\"b\"] -= learning_rate * self.db\n","        ### END CODE HERE ###"]},{"cell_type":"code","execution_count":3,"id":"aeac387c","metadata":{"id":"aeac387c"},"outputs":[],"source":["class Activation():\n","    def __init__(self, activation_function, loss_function, alpha=None, gamma=None):\n","        self.activation_function = activation_function\n","        self.loss_function = loss_function\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, Z):\n","        if self.activation_function == \"sigmoid\":\n","            \"\"\"\n","            Implements the sigmoid activation in numpy\n","\n","            Arguments:\n","            Z -- numpy array of any shape\n","            self.cache -- stores Z as well, useful during backpropagation\n","\n","            Returns:\n","            A -- output of sigmoid(z), same shape as Z\n","            \"\"\"\n","\n","\n","            ### START CODE HERE ### (≈ 8 lines of code)\n","            A = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n","            self.cache = Z\n","            ### END CODE HERE ###\n","\n","            return A\n","        elif self.activation_function == \"relu\":\n","            \"\"\"\n","            Implement the RELU function in numpy\n","            Arguments:\n","            Z -- numpy array of any shape\n","            self.cache -- stores Z as well, useful during backpropagation\n","            Returns:\n","            A -- output of relu(z), same shape as Z\n","\n","            \"\"\"\n","\n","            ### START CODE HERE ### (≈ 2 lines of code)\n","            A = np.maximum(0, Z)\n","            self.cache = Z\n","            ### END CODE HERE ###\n","\n","            assert(A.shape == Z.shape)\n","\n","            return A\n","        elif self.activation_function == \"softmax\":\n","            \"\"\"\n","            Implements the softmax activation in numpy\n","\n","            Arguments:\n","            Z -- np.array with shape (n, C)\n","            self.cache -- stores Z as well, useful during backpropagation\n","\n","            Returns:\n","            A -- output of softmax(z), same shape as Z\n","            \"\"\"\n","\n","            ### START CODE HERE ### (≈ 3 lines of code)\n","            b = np.max(Z, axis=1, keepdims=True)\n","            exp_Z = np.exp(Z - b)\n","            A = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n","            self.cache = (Z)\n","            ### END CODE HERE ###\n","\n","            return A\n","        else:\n","            assert 0, f\"you're using undefined activation function {self.activation_function}\"\n","\n","\n","    def backward(self, dA=None, Y=None):\n","        if self.activation_function == \"sigmoid\":\n","            \"\"\"\n","            Implement the backward propagation for a single SIGMOID unit.\n","            Arguments:\n","            dA -- post-activation gradient, of any shape\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the loss with respect to Z\n","            \"\"\"\n","\n","            ### START CODE HERE ### (≈ 9 lines of code)\n","            Z = self.cache\n","            sigmoid_Z = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n","            dZ = dA * sigmoid_Z * (1 - sigmoid_Z)\n","            ### END CODE HERE ###\n","\n","            assert (dZ.shape == Z.shape)\n","\n","            return dZ\n","\n","        elif self.activation_function == \"relu\":\n","            \"\"\"\n","            Implement the backward propagation for a single RELU unit.\n","            Arguments:\n","            dA -- post-activation gradient, of any shape\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the loss with respect to Z\n","            \"\"\"\n","\n","            ### START CODE HERE ### (≈ 3 lines of code)\n","            Z = self.cache\n","            dZ = np.where(Z > 0, dA, 0)\n","            ### END CODE HERE ###\n","\n","            assert (dZ.shape == Z.shape)\n","\n","            return dZ\n","\n","        elif self.activation_function == \"softmax\" and self.loss_function == 'cross_entropy':\n","            \"\"\"\n","            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n","            Arguments:\n","            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the cost with respect to Z\n","            \"\"\"\n","\n","            # GRADED FUNCTION: softmax_CCE_backward\n","            ### START CODE HERE ### (≈ 3 lines of code)\n","            Z = self.cache\n","\n","            b = np.max(Z, axis=1, keepdims=True)\n","            exp_Z = np.exp(Z - b)\n","            #sum_exp_Z = np.sum(exp_Z, axis=1, keepdims=True)\n","            s = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n","\n","            dZ = s - Y\n","            ### END CODE HERE ###\n","\n","            assert (dZ.shape == self.cache.shape)\n","\n","            return dZ\n","        elif self.activation_function == \"softmax\" and self.loss_function == 'focal_loss':\n","            \"\"\"\n","            Implement the backward propagation for a [SOFTMAX->FOCAL LOSS] unit.\n","            Arguments:\n","            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","            self.cache -- 'Z' where we store for computing backward propagation efficiently\n","            Returns:\n","            dZ -- Gradient of the cost with respect to Z\n","            alpha -- weighting factors correspond to each class, shape: (C,)\n","            gamma -- modulating factor, a float\n","            \"\"\"\n","\n","            # FUNCTION: softmax_focalLoss_backward\n","            ## START CODE HERE ### (≈ 10 lines of code)\n","            Z = self.cache\n","\n","            exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n","            s = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n","\n","            m = Y.shape[0]\n","            n = Z.shape[1]\n","\n","            gamma = self.gamma\n","            alpha = self.alpha\n","\n","            dZ = np.zeros_like(Z, dtype=float)\n","            for i in range(m):\n","                for j in range(n):\n","                    p_it = s[i, np.argmax(Y[i])] # p_it = s[i,t]\n","                    alpha_t = alpha[np.argmax(Y[i])] # alpha_t = alpha[t]\n","                    is_true_label = (j == np.argmax(Y[i]))\n","                    if is_true_label:\n","                        dZ[i, j] = alpha_t * (gamma * (1 - p_it)**(gamma-1) * (np.log(p_it + 1e-5)) * (p_it - p_it**2) - (1 - p_it)**(gamma) * (1 - p_it))\n","                    else:\n","                        dZ[i, j] = alpha_t * (gamma * (1 - p_it)**(gamma-1) * (np.log(p_it + 1e-5)) * (-s[i, j] * p_it) - (1 - p_it)**(gamma) * (-s[i, j]))\n","\n","            ## END CODE HERE ###\n","\n","            assert (dZ.shape == self.cache.shape)\n","\n","            return dZ"]},{"cell_type":"code","execution_count":4,"id":"f38e1807","metadata":{"id":"f38e1807"},"outputs":[],"source":["class Model():\n","    def __init__(self, config):\n","        self.config = config\n","        self.units = config.layers_dims\n","        self.activation_functions = config.activation_fn\n","        self.loss_function = config.loss_function\n","        self.alpha = config.alpha\n","        self.gamma = config.gamma\n","        self.initialize_parameters()\n","        self.check = True\n","\n","    def initialize_parameters(self):\n","        \"\"\"\n","        Arguments:\n","        self.units -- number of nodes/units for each layer, starting from the input dimension and ending with the output dimension (i.e., [4, 4, 1])\n","        self.activation_functions -- activation functions used in each layer (i.e, [\"relu\", \"sigmoid\"])\n","        self.loss_function -- [\"cross_entropy\", \"focal_loss\"]\n","        self.alpha -- weighting factors used by focal loss correspond to each class, shape: (C,)\n","        self.gamma -- a float, used by focal loss\n","        \"\"\"\n","        self.linear = []        # a list to store the dense layers when initializing the model\n","        self.activation = []    # a list to store the activation function layers when initializing the model\n","\n","        \n","        # FUNCTION: model_initialize_parameters\n","        ### DO NOT MODIFY THIS PART ###\n","        for i in range(len(self.units)-1):\n","            dense = Dense(self.units[i], self.units[i+1], i)\n","            self.linear.append(dense)\n","\n","        for i in range(len(self.activation_functions)):\n","            self.activation.append(Activation(self.activation_functions[i], self.loss_function, self.alpha, self.gamma))\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Arguments:\n","        X -- input data: shape (n, f)\n","\n","        Returns:\n","        A -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (n, C)\n","        \"\"\"\n","        A = X\n","\n","        # GRADED FUNCTION: model_forward\n","        ### START CODE HERE ### (≈ 4 lines of code)\n","        for linear_layer, activation_layer in zip(self.linear, self.activation):\n","            Z = linear_layer.forward(A)\n","            A = activation_layer.forward(Z)\n","        ### END CODE HERE ###\n","\n","        return A\n","\n","    def backward(self, AL=None, Y=None):\n","        \"\"\"\n","        Arguments:\n","        For multi-class classification,\n","        AL -- output of L-layer neural network, probability vector corresponding to your label predictions, shape (n, C)\n","        Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n","                                      in a Rock-Paper-Scissors, shape: (n, C)\n","\n","        Returns:\n","        dA_prev -- post-activation gradient\n","        \"\"\"\n","\n","        L = len(self.linear)\n","        C = Y.shape[1]\n","\n","        # assertions\n","        warning = 'Warning: only the following 4 combinations are allowed! \\n \\\n","                    1. binary classification: sigmoid + cross_entropy) \\n \\\n","                    2. binary classification: softmax + focal_loss) \\n \\\n","                    3. multi-class classification: softmax + cross_entropy) \\n \\\n","                    4. multi-class classification: softmax + focal_loss)'\n","        assert self.loss_function in [\"cross_entropy\", \"focal_loss\"], \"you're using undefined loss function!\"\n","        if Y.shape[1] <= 2:                                 # in binary classification\n","            if self.loss_function == \"cross_entropy\":\n","                assert self.activation_functions[-1] == 'sigmoid', warning\n","                assert self.units[-1] == 1, \"you should set last dim to 1 when using sigmoid + cross_entropy in binary classification!\"\n","            elif self.loss_function  == \"focal_loss\":\n","                assert self.activation_functions[-1] == 'softmax', warning\n","                assert self.units[-1] == 2, \"you should set last dim to 2 when using softmax + focal_loss in binary classification!\"\n","        else:                                               # in multi-class classification\n","            assert self.activation_functions[-1] == 'softmax', warning\n","            assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n","\n","        # FUNCTION: model_backward\n","        ### START CODE HERE ### (≈ 20 lines of code)\n","\n","        if self.activation_functions[-1] == \"sigmoid\":\n","            if self.loss_function == 'cross_entropy':\n","                # Initializing the backpropagation\n","                dAL = - (np.divide(Y, AL + 1e-5) - np.divide(1 - Y, 1 - AL + 1e-5))\n","\n","                # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n","                dZ = self.activation[-1].backward(dA=dAL, Y=Y)\n","                dA_prev = self.linear[-1].backward(dZ)\n","        elif self.activation_functions[-1] == \"softmax\":\n","            # Initializing the backpropagation\n","            dZ = self.activation[-1].backward(dA=AL, Y=Y)\n","\n","            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n","            dA_prev = self.linear[-1].backward(dZ)\n","\n","        # Loop from l=L-2 to l=0\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n","        for l in range(L-2, -1, -1):\n","            activation_layer = self.activation[l]\n","            linear_layer = self.linear[l]\n","\n","            # Backward pass through activation layer (RELU)\n","            dZ = activation_layer.backward(dA=dA_prev, Y=None)  # You need to provide Y for the softmax layer\n","\n","            # Backward pass through linear layer\n","            dA_prev = linear_layer.backward(dZ)\n","        ### END CODE HERE ###\n","\n","        return dA_prev\n","\n","    def update(self, learning_rate):\n","        \"\"\"\n","        Arguments:\n","        learning_rate -- step size\n","        \"\"\"\n","\n","        L = len(self.linear)\n","\n","        # FUNCTION: model_update_parameters\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        for l in range(L):\n","            self.linear[l].update(learning_rate)\n","        ### END CODE HERE ###"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.11.5 ('biopsy')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"bc07a8c6aa785ccbb5cb0815abffaffb139b111aca417c4ffe9bf221bae76ba2"}}},"nbformat":4,"nbformat_minor":5}
